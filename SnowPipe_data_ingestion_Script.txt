/* automate data ingestion from aws s3 into snowflake table using snowpipe */

/* 
What is Snowpipe?
-- Snowpipe enables loading data from files as soon as theyâ€™re available in a stage. 
-- This means you can load data from files in micro-batches, making it available to users within minutes, 
   rather than manually executing COPY statements on a schedule to load larger batches.
*/

-- Create table to load CSV data
CREATE or replace TABLE HEALTHCARE_CSV(
    AVERAGE_COVERED_CHARGES    NUMBER(38,6)  
   ,AVERAGE_TOTAL_PAYMENTS    NUMBER(38,6)  
   ,TOTAL_DISCHARGES    NUMBER(38,0)  
   ,BACHELORORHIGHER    NUMBER(38,1)  
   ,HSGRADORHIGHER    NUMBER(38,1)  
   ,TOTALPAYMENTS    VARCHAR(128)  
   ,REIMBURSEMENT    VARCHAR(128)  
   ,TOTAL_COVERED_CHARGES    VARCHAR(128) 
   ,REFERRALREGION_PROVIDER_NAME    VARCHAR(256)  
   ,REIMBURSEMENTPERCENTAGE    NUMBER(38,9)  
   ,DRG_DEFINITION    VARCHAR(256)  
   ,REFERRAL_REGION    VARCHAR(26)  
   ,INCOME_PER_CAPITA    NUMBER(38,0)  
   ,MEDIAN_EARNINGSBACHELORS    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_GRADUATE    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_HS_GRAD    NUMBER(38,0)  
   ,MEDIAN_EARNINGSLESS_THAN_HS    NUMBER(38,0)  
   ,MEDIAN_FAMILY_INCOME    NUMBER(38,0)  
   ,NUMBER_OF_RECORDS    NUMBER(38,0)  
   ,POP_25_OVER    NUMBER(38,0)  
   ,PROVIDER_CITY    VARCHAR(128)  
   ,PROVIDER_ID    NUMBER(38,0)  
   ,PROVIDER_NAME    VARCHAR(256)  
   ,PROVIDER_STATE    VARCHAR(128)  
   ,PROVIDER_STREET_ADDRESS    VARCHAR(256)  
   ,PROVIDER_ZIP_CODE    NUMBER(38,0)  
);


1. An AWS IAM user created for your Snowflake account is associated with an IAM role you configure via a trust relationship.
2. The role is granted limited access to an S3 bucket through IAM policies you configure


Step 1: Create a S3 Bucket and Configure S3 Bucket Access Permissions
-------------------------------------------------
Create a S3 Bucket --> it should be unique accross AWS Accounts in the world (all small letters)
AWS Access Control Requirements
Snowflake requires the following permissions on an S3 bucket and folder to be able to access files in the folder (and any sub-folders):
s3:GetObject  -- get the file
s3:GetObjectVersion  - get the file version
s3:ListBucket  - View the bucket Details
s3:PutObject        -- upload a file
s3:DeleteObject -- Delete a file`
s3:DeleteObjectVersion  -- Delete a file version

Creating an IAM Policy
--------------------------------
The following step-by-step instructions describe how to configure access permissions for Snowflake in your AWS Management Console so that you can use an S3 bucket to load and unload data:
1.Log into the AWS Management Console.
2.From the home dashboard, choose Identity & Access Management (IAM):
3.Choose Account settings from the left-hand navigation pane.
4.Expand the Security Token Service Regions list, find the AWS region corresponding to the region where your account is located, and choose Activate if the status is Inactive.
5.Choose Policies from the left-hand navigation pane.
6.Click Create Policy:
7.Click the JSON tab.
Add a policy document that will allow Snowflake to access the S3 bucket and folder.
Copy the Below JSON Script and Paste it in IAM Policy JSON Window / Tab (Dont copy the Line -------)
------------------------------------------------------------------------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
              "s3:PutObject",
              "s3:GetObject",
              "s3:GetObjectVersion",
              "s3:DeleteObject",
              "s3:DeleteObjectVersion"
            ],
            "Resource": "arn:aws:s3:::sftraining/*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::sftraining",
            "Condition": {
                "StringLike": {
                    "s3:prefix": [
                        "*"
                    ]
                }
            }
        }
    ]
}
--------------------------------------------------
9.Click Review policy.
10. Enter the policy name (e.g. snowflake_access) and an optional description. 
Then, click Create policy to create the policy.

Creating an IAM Role -- Configure the AWS IAM Role to Allow Access to the Stage
-------------------------------------------------------------------------------------
1.Log into the AWS Management Console.
2.From the home dashboard, choose Identity & Access Management (IAM)
3.Choose Roles from the left-hand navigation pane
4.Click on Create Role Button
5.Select Another AWS account as the trusted entity type.
6.In the Account ID field, enter your own AWS account ID. Later, you will modify the 
  trusted relationship and grant access to Snowflake. 
  An external ID is required to grant access to your AWS resources (i.e. S3) to a 
  third party (i.e. Snowflake in this case) later in these instructions.
7.Select the Require external ID option. Enter a dummy ID such as 0000. Later, you will modify the trusted relationship and specify the external ID for your Snowflake stage.
8.Click the Next button.
9.Locate the policy you created above and select this policy.
10.Click the Next button.
11.Enter a name and description for the role, and click the Create role button.
12.You have now created an IAM policy for a bucket, created an IAM role, and attached the policy to the role.
13.Save the Role ARN value located on the role summary page. In the next step, you will create a Snowflake stage that references this role as the security credentials.


--------------------------------------xxxxxxxxxxxxxxxx-----------------------------------

--Create integration object for external stage
create or replace storage integration s3_int
  type = external_stage
  storage_provider = s3
  enabled = true
  storage_aws_role_arn = 'arn:aws:iam::435098453023:role/snowflake-role'
  storage_allowed_locations = ('s3://testsnowflake/snowflake/', 's3://testxyzsnowflake/');

--Describe integration object to fetch external_id and to be used in s3
DESC INTEGRATION s3_int;

create or replace file format demo_db.public.csv_format
                    type = csv
                    field_delimiter = '|'
                    skip_header = 1
                    null_if = ('NULL', 'null')
                    empty_field_as_null = true;
                    
create or replace stage demo_db.public.ext_csv_stage
  URL = 's3://testsnowflake/snowflake/csv'
  STORAGE_INTEGRATION = s3_int
  file_format = demo_db.public.csv_format;


--create pipe to automate data ingestion from s3 to snowflake
create or replace pipe demo_db.public.mypipe auto_ingest=true as
copy into healthcare_csv
from @demo_db.public.ext_csv_stage
on_error = CONTINUE;

---list pipe object and copy sqs 
show pipes;  ---- copy sqs

---validate whether data is inserted or not 
select * from healthcare_csv;


